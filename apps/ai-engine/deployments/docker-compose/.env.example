# ==============================================================================
# AI ENGINE - ENVIRONMENT CONFIGURATION
# ==============================================================================
# Copy this file to .env and fill in the required values.

# ------------------------------------------------------------------------------
# SECTION 1: DEPLOYMENT SCENARIO GUIDE (Read this first!)
# ------------------------------------------------------------------------------
#
# This service is designed to be highly flexible. Uncomment ONE of the scenarios
# below by setting the EMBEDDING_PROVIDER and COMPLETION_PROVIDER variables in
# Section 2.
#
# --- SCENARIO 1: Fully Local Development (Default) ---
#   - Cost: $0
#   - Use Case: Development, testing, and full privacy.
#   - Settings: EMBEDDING_PROVIDER=ollama, COMPLETION_PROVIDER=ollama
#
# --- SCENARIO 2: High-Speed Prototyping ---
#   - Cost: Low (Groq is very cost-effective)
#   - Use Case: The best "wow" demo experience. Blazing fast answers.
#   - Settings: EMBEDDING_PROVIDER=ollama, COMPLETION_PROVIDER=groq
#
# --- SCENARIO 3: Enterprise Cloud-Native ---
#   - Cost: High (OpenAI API costs)
#   - Use Case: Production deployments requiring a fully managed, auditable cloud pipeline.
#   - Settings: EMBEDDING_PROVIDER=openai, COMPLETION_PROVIDER=openai
#
# --- SCENARIO 4: Mixed Enterprise (Speed-Optimized) ---
#   - Cost: High
#   - Use Case: Production deployments that need OpenAI's embedding quality but Groq's completion speed.
#   - Settings: EMBEDDING_PROVIDER=openai, COMPLETION_PROVIDER=groq
#
# --- SCENARIO 5: Cost-Effective Testing ---
#   - Cost: Very Low (OpenRouter is very cost-effective)
#   - Use Case: Development and testing with minimal cloud costs.
#   - Settings: EMBEDDING_PROVIDER=ollama, COMPLETION_PROVIDER=openrouter
#
# # --- SCENARIO 6: Ultra-Fast Everything (NOT SUPPORTED) ---
#   - Cost: Medium (Groq for both embeddings and completions)
#   - Use Case: Maximum speed for both embeddings and completions.
#   # - Settings: EMBEDDING_PROVIDER=groq, COMPLETION_PROVIDER=groq (NOT SUPPORTED)
#
# # --- SCENARIO 7: Cost-Effective Everything (NOT SUPPORTED) ---
#   - Cost: Very Low (OpenRouter for both embeddings and completions)
#   - Use Case: Maximum cost savings with cloud processing.
#   # - Settings: EMBEDDING_PROVIDER=openrouter, COMPLETION_PROVIDER=openrouter (NOT SUPPORTED)
#
# --- SCENARIO 8: Local Embeddings + Cloud Completions ---
#   - Cost: Medium (OpenAI completions only)
#   - Use Case: Privacy for embeddings, quality for completions.
#   - Settings: EMBEDDING_PROVIDER=ollama, COMPLETION_PROVIDER=openai
#
# ------------------------------------------------------------------------------
# SECTION 2: CORE AI PROVIDER SELECTION
# ------------------------------------------------------------------------------
# Choose your providers based on the scenarios above.

# Embedding provider selection (ollama, openai, groq, or openrouter)
EMBEDDING_PROVIDER=ollama

# Completion provider selection (ollama, groq, openai, or openrouter)
COMPLETION_PROVIDER=groq

# ------------------------------------------------------------------------------
# SECTION 3: PROVIDER-SPECIFIC CONFIGURATION
# ------------------------------------------------------------------------------

# -- Ollama Settings (for local models) --
OLLAMA_BASE_URL=http://host.docker.internal:11434
EMBEDDING_MODEL=nomic-embed-text
COMPLETION_MODEL=llama3:8b

# -- Groq Settings (for high-speed inference) --
GROQ_API_KEY=your-groq-api-key-here
GROQ_MODEL=llama-3.1-8b-instant # or llama3-8b-8192

# -- OpenAI Settings (for enterprise compatibility) --
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-3.5-turbo

# -- OpenRouter Settings (uses OpenAI compatibility) --
# Note: OpenRouter uses the same configuration as OpenAI above
# No separate OPENROUTER_API_KEY or OPENROUTER_MODEL needed
# 
# To use OpenRouter:
# 1. Set EMBEDDING_PROVIDER=openrouter or COMPLETION_PROVIDER=openrouter
# 2. Use OPENAI_API_KEY with your OpenRouter API key
# 3. Set OPENAI_BASE_URL=https://openrouter.ai/api/v1
# 4. Set OPENAI_MODEL to your desired model (e.g., deepseek/deepseek-chat)

# ------------------------------------------------------------------------------
# SECTION 4: SERVICE & INFRASTRUCTURE CONFIGURATION
# ------------------------------------------------------------------------------

# -- AI Engine Server Settings --
AI_ENGINE_PORT=8000
WEAVIATE_PORT=8080
SERVER_ENV=development
SERVER_HOST=localhost

# -- Vector Database Settings --
WEAVIATE_URL=http://weaviate:8080 # Container name for Docker
WEAVIATE_API_KEY=

# -- General Settings --
LOG_LEVEL=info
CORS_ENABLED=true
CORS_ORIGINS=*

# ------------------------------------------------------------------------------
# SECTION 5: QUICK START EXAMPLES
# ------------------------------------------------------------------------------
#
# For quick testing, uncomment one of these complete configurations:
#
# # Local Development (Free)
# EMBEDDING_PROVIDER=ollama
# COMPLETION_PROVIDER=ollama
# OLLAMA_BASE_URL=http://host.docker.internal:11434
#
# # High-Speed Demo (Low Cost)
# EMBEDDING_PROVIDER=ollama
# COMPLETION_PROVIDER=groq
# GROQ_API_KEY=your-groq-api-key-here
# OLLAMA_BASE_URL=http://host.docker.internal:11434
#
# # Enterprise Production (High Cost)
# EMBEDDING_PROVIDER=openai
# COMPLETION_PROVIDER=openai
# OPENAI_API_KEY=your-openai-api-key-here
#
# # Cost-Effective Testing (Very Low Cost)
# EMBEDDING_PROVIDER=ollama
# COMPLETION_PROVIDER=openrouter
# OPENAI_API_KEY=your-openrouter-api-key-here
# OPENAI_BASE_URL=https://openrouter.ai/api/v1
# OPENAI_MODEL=deepseek/deepseek-chat
# OLLAMA_BASE_URL=http://host.docker.internal:11434
#
# # Ultra-Fast Everything (Medium Cost)
# EMBEDDING_PROVIDER=groq
# COMPLETION_PROVIDER=groq
# GROQ_API_KEY=your-groq-api-key-here
#
# # Cost-Effective Everything (Very Low Cost)
# EMBEDDING_PROVIDER=openrouter
# COMPLETION_PROVIDER=openrouter
# OPENAI_API_KEY=your-openrouter-api-key-here
# OPENAI_BASE_URL=https://openrouter.ai/api/v1
# OPENAI_MODEL=deepseek/deepseek-chat

# ==============================================================================
# IMPORTANT: EMBEDDING PROVIDER LIMITATIONS
# ==============================================================================
#
# Groq and OpenRouter do NOT support embeddings.
# Use them only for completions in hybrid configurations:
#
# ✅ Supported Embedding Providers: ollama, openai
# ✅ Supported Completion Providers: ollama, groq, openai, openrouter
#
# Recommended hybrid configurations:
# - Ollama embeddings + Groq/OpenRouter completions
# - OpenAI embeddings + any completion provider

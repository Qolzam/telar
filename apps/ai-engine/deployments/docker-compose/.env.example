# Development Environment Configuration for AI Engine
# Copy this file to .env and modify values as needed

# Server Configuration
SERVER_PORT=8000
SERVER_HOST=localhost
SERVER_ENV=development

# Legacy provider field (for backward compatibility)
LLM_PROVIDER=ollama

# Completion provider selection (ollama or groq)
# Set to "groq" for lightning-fast completions, "ollama" for all-local setup
COMPLETION_PROVIDER=groq

# Ollama Configuration (Required - Always used for embeddings)
OLLAMA_BASE_URL=http://host.docker.internal:11434
EMBEDDING_MODEL=nomic-embed-text
COMPLETION_MODEL=llama3:8b

# Groq Provider Settings (for high-performance inference)
GROQ_API_KEY=your-groq-api-key-here
GROQ_MODEL=llama-3.1-8b-instant


# OpenAI Provider Settings (for enterprise compatibility)
OPENAI_API_KEY=your-openai-api-key-here

# Vector Database Configuration
WEAVIATE_URL=http://localhost:8080
WEAVIATE_API_KEY=

# Logging Configuration
LOG_LEVEL=info

# CORS Configuration
CORS_ENABLED=true
CORS_ORIGINS=*

# Performance Tuning
FIBER_PREFORK=false
FIBER_CASE_SENSITIVE=false
FIBER_STRICT_ROUTING=false